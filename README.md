<picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://docs.agentbase.space/_static/ell2a-wide-dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://docs.agentbase.space/_static/ell2a-wide-light.png">
  <img alt="ell2a logo that inverts based on color scheme" src="https://docs.agentbase.space/_static/ell2a-wide.png">
</picture>

--------------------------------------------------------------------------------

[![Documentation Status](https://img.shields.io/badge/documentation-go)](https://docs.agentbase.space/) [![Install](https://img.shields.io/badge/get_started-blue)](https://docs.agentbase.space/installation) [![Discord](https://dcbadge.limes.pink/api/server/vWntgU52Xb?style=flat)](https://discord.gg/vWntgU52Xb) [![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/wgussml)](https://x.com/wgussml)

```bash

pip install -U "ell2a-ai[all]"
```

`ell2a` is a lightweight, functional prompt engineering framework built on a few core principles:

### 1. Prompts are programs, not strings

Prompts aren't just strings; they are all the code that leads to strings being sent to a language model. In `ell2a` we think of one particular way of using a language model as a discrete subroutine called a **language model program**.

```python
import ell2a

@ell2a.simple(model="gpt-4o")
def hello(world : str):
    """You are a helpful assistant that writes in lower case.""" # System Message
    return f"Say hello to {world[::-1]} with a poem."    # User Message

hello("sama")
```

![alt text](https://docs.agentbase.space/_static/gif1.webp)

### 2. Prompts are actually parameters of a machine learning model

The process of prompt engineering involves many iterations, similar to the optimization processes in machine learning. Because LMPs are just functions, `ell2a` provides rich agenting for this process.

![ell2a demonstration](https://docs.agentbase.space/_static/versions_small.webp)

`ell2a` provides **automatic versioning and serialization of prompts** through static and dynamic analysis and  `gpt-4o-mini` **autogenerated commit messages** directly to a *local store*. This process is similar to `checkpointing` in a machine learning training loop, but it doesn't require any special IDE or editor - it's all done with regular Python code.

### 3. Tools for monitoring, versioning, and visualization

Prompt engineering goes from a dark art to a science with the right agents. **Ell2a Studio is a local, open source agent for prompt version control, monitoring, visualization**. With Ell2a Studio you can empiricize your prompt optimization process over time and catch regressions before it's too late.

<picture>
  <source srcset="https://docs.agentbase.space/_static/ell2a_studio_better.webp" type="image/webp">
  <img src="docs/src/_static/ell2a_studio_better.webp" alt="ell2a studio demonstration">
</picture>

```bash
ell2a-studio --storage ./logdir 
```

### 4. Multimodality should be first class

LLMs can process and generate various types of content, including text, images, audio, and video. Prompt engineering with these data types should be as easy as it is with text.

```python
from PIL import Image
import ell2a


@ell2a.simple(model="gpt-4o", temperature=0.1)
def describe_activity(image: Image.Image):
    return [
        ell2a.system("You are VisionGPT. Answer <5 words all lower case."),
        ell2a.user(["Describe what the person in the image is doing:", image])
    ]

# Capture an image from the webcam
describe_activity(capture_webcam_image()) # "they are holding a book"
```

![ell2a demonstration](https://docs.agentbase.space/_static/multimodal_compressed.webp)

`ell2a` supports rich type coercion for multimodal inputs and outputs. You can use PIL images, audio, and other multimodal inputs inline in `Message` objects returned by LMPs.

### ...and much more

Read more in the [docs](https://docs.agentbase.space/)!

## Installation

To install `ell2a` and `ell2a studio`, you can use pip. Follow these steps:

1. Open your terminal or command prompt.
2. Run the following command to install the `ell2a-ai` package from PyPI:

   ```bash
   pip install ell2a-ai
   ```

3. Verify the installation by checking the version of `ell2a`:

   ```bash
   python -c "import ell2a; print(ell2a.__version__)"
   ```

This will install both `ell2a` and `ell2a studio` on your system, allowing you to start using the agents for prompt engineering and visualization.

## Next Steps

Explore the [documentation](https://docs.agentbase.space/) to learn more about `ell2a` and its features. Follow the [Getting Started guide](https://docs.agentbase.space/getting_started.html) to create your first Language Model Program. Join our [Discord community](https://discord.gg/vWntgU52Xb) to connect with other users and get support.
