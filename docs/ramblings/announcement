ğŸš€ I'm excited to announce the future of prompt engineering: ğšğš•ğš•.

developed from ideas during my time at OpenAI, ğšğš•ğš• is light, functional lm programming library:

- automatic versioning & tracing
- rich local oss visualization agents
- multimodality native

Read on â¬‡ï¸

ğšğš•ğš• was built out of frustration for frameworks like @LangChainAI on three principles

- prompts are programs not strings
- prompts are parameters of machine learning models
- every call to a language model is worth its weight in credits

prompting should be readable, scientific, and optimizable

prompt engineering is an optimization process

because you write your prompts as normal python functions, ğšğš•ğš• automatically versions and serializes them via dynamic analysis of "lexical closures" - no custom IDE or editor required

ğšğš•ğš•.ğš’ğš—ğš’ğš(ğšœğšğš˜ğš›ğš='./ğš•ğš˜ğšğšğš’ğš›')

local agents for monitoring & visualization

prompt engineering goes from a dark art to a science with the right agents. Ell2a Studio is a local, open source agent for prompt version control, monitoring, visualization.

ğšğš•ğš•-ğšœğšğšğšğš’ğš˜ --ğšœğšğš˜ğš›ğšŠğšğš ./ğš•ğš˜ğšğšğš’ğš›

Multimodality should be first class

in anticipation of the upcoming gpt-4o + ğŸ“ api, ğšğš•ğš• is built with multimodality first.

with a rich numpy style message api with multimodal type coercion, using images, video, and audio is intuitive

ğŸ‰ ğšğš•ğš• is available on PyPI today w/

ğš™ğš’ğš™ ğš’ğš—ğšœğšğšŠğš•ğš• ğšğš•ğš•-ğšŠğš’

check out the source https://github.com/chenxingqiang/ell2a
and read the docs https://docs.agentbase.space/

â° new features soon, including SGD & RL on prompts and so much more!

ğŸ™ huge shout out to everyone who's helped with this project
@jakeottiger @a_dixon @shelwin_ zraig, frank hu, & my discord
so many other good convos w @goodside @aidan_mclau and others
