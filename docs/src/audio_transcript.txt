
Ramble:

The following is a set of notes regarding The design philosophy, description of uses for. And features of LA prompt engineering library and associated toolkit for developing Complex. Combinations of calls to multimodal language models. L is a Python Based prompt engineering Based on a few key design principles. 

1. Prompts are programs not strings period. The set of code that leads to a prompt being Sent to a language model or a list of messages being sent to a chat based language model Are created in a programming interface, not by just creating format strings, but by A pot Mongset of Instructions and subroutines that lead to the formation of a string being sent to a language model And therefore L is built on a fundamental unit called the language model program. Period. Each language model program is a fully encapsulated function. Which produces either a Bring prompt or a list of messages ie, assistant prompt and A user prompt previous assistant messages and so on, that are then sent to various different. multimodal language models By forcing implementers to encounter. to encapsulate the production of a prompt string or prompt messages in a. single Function or method. We create an extremely clean interface for users of Prompt engineered Language model calls in that. the user Of the prompt engineered language model. Call need only be aware of the required data. Specified to the language model program parentheses prompt. Forced encapsulation Of prompts ie language model programs. Allows us to implement the next set of design principles for ell

2. Prompt engineering deserves the same high quality Tooling and design patterns. As machine learning engineering. In practice The process of prompt engineering involves many iterations on the program that constitutes a prompt. IE, changing the formatting strings Updating parameters to the language model, adding various different rules to the sister prompt Processing input data differently and so on. Period. This process is akin to what in machine learning research we call graduate student ascent. A fun play on words for The optimization process, gradient descent, wherein Developing new neural architectures for deep learning models involves a lot of tweaking of the model Definition code implied torch. Over many different training rooms In machine learning We have a rich set of tooling for this process involving saving model checkpoints and version control of model hyperparameters over time period. The current state of prompt engineering Is either versionless The prompt engineer sits in their id and modifies various different prompts and processing code. Repeatedly testing by quotes vibes quotes. To see whether or not the prompt is improved or is highly versioned, requiring that the prompt engineer use a custom IDE To ensure that some sort of version control or database will store that version of their prompt. This often leads to forcing very opinionated structures on a prompt engineer, such as using ginja formatting strings, things like this overall. make learning a prompt engineering library cumbersome, unintuitive And furthermore later modifications to prompts. Are difficult for different practitioners because the code is oftentimes unreadable in these prompt engineering frameworks. We believe that Or sorry, I guess we don't believe that. But in L the idea. Is that Prompt engineer shouldn't have to compromise on these aspects, period. By viewing prompts as discrete programs defined by. A functional interface where a prompt is just a return string. Or data type of a function Automatically allows us to serialize. These language model programs And Change through static and dynamic analysis period. in L every time you define and run a language model program we compute Something called the lexical closure, which is. a Formalism For all of the source code that is required to run a function. For example, if I have a function, F of X, which depends on a global variable That's in some source file on my project. The lexical closure is all of the source code of the body of that function. It's functional signature its parameters and its name. And then any global variable that it depends on. Every time you define a language model program in L. We compute this lexical closure and then. automatically generate a version for the function We then can track all of the invocations. Of a language model program, ie a prompt Being called to a language model by their version so that the prompt practitioner can compute metric space on the version. Track changes over time due to changes in model or prompt code And automatically generate Commit messages between versions At some later point in the documentation, we'll show this through L studio OK

3. Every call to a language model is worth its Weight in credits. You know, because we require that all indications of language model programs be. encapsulated in a functional call, we're able to build a tensor board like Experience Prompt engineering practitioners. By simply installing a local free to use Walking utility in your code. Not only are prompts automatically versioned and serialized, but so too are all of the invocations of those prompts, so that for later use Propped invocations can be fine tuned on to save money using free fine tuning Apis. Rhf, comparison details can be constructed for continually Iterative improvements. Of the use case which the prompt was engineered to solve. Good regressions can be detected. And so on. Because this design philosophy L has these things called L stores, which support sqlite storage, postcrest storage And we're in the progress of building a simple. Won't be like service for Prompt engineers to store The versions of all of their prompts, as well as all the invocations of their prompts Online during the prompt engineering process and during production employers. Because we have this design philosophy we have built within L some various tools which enable you to track when language models are called and trace The outputs of those language models, as they're used by other language model programs in your code base. We'll have a whole section on tracing later. 

4. Making a demo with a one shot prompt. It's easy, but solving Real world use cases requires test time compute. In practice Propped engineering involves Multiple calls to a language model. Often test time compute can improve the performance of a language model programming system. By several percentage points by using best event sampling Defining model based critics Doing prompt optimization and so on. Period, the functional structure of L is so that this process is. Extremely simple. Good by viewing indications of prompts as functions prompts to be mix and match, composed Reused and distributed In a very clean and readable way.

> Note to claude Ok, so these are some of the design principles behind L, but this is not necessarily the way I want that to present in the docDude, just things you need to keep in mind when we were kind of structuring. A really beautiful set of documentation for this. language model programming library called L. By the way, L doesn't stand for anything. It's just Ell, which Plan words for the letter L standing for language models and so on. 

OK, so let's talk about how to use L and we're going to go through All of the use cases and motivations and notes. And so on now 


So L basically works by. you defining a language model program, which is just a callable in python that returns either a string or a list of messages Once you've defined this callable, which is basically your prompt, you wrap that callable with a decorator. And there are two types of decorators. There's. L.simplewhich. Avoid some of the most annoying parts of These language model api python bindings. Ie that the return type from the api is this very nested And structured object containing All the metadata about the coal, as well as the messages and the content and the structure of content markup And the reason for and this is kind of a side note, the reason for this return typing so complicated is because most of these language model API bindings are generated automatically through stainless from an open API spec and they. can't do much on the developer facing side to make the return type of their API very clean period at the end of the day when you're interacting with these language model Apis were passing strings around. So what L simple does is it Basically makes it extremely easy to interact with the return type of the api on it. And once you wrap a language model program or a prompt in this L dot simple decorator You specify the model, you specify all of the api parameters, such as the. Temperature, the number of generations, the max tokens, the logic biases and any other useful parameters you might specify to open AI and through optic. Xai, et cetera models. And then that function that you've wrapped now can be called just like any normal python function. And when you call it, it basically takes the Stringy returned And the dock string. And you can see this from the examples. And you'll structure the Doc's note to claude that you'll structure the docs here to be cleaned. and make sense It'll take the string that you returned. And use that as the user prompt. And the dock string of the function, it'll use that as the system prompt, or it'll just take them list of messages that you return and pass them to the api. It's a sort of chat completions api for when those language models and when you call the function that you've wrapped in L dot simple, it just returns a string. And the string is what the model replied with, right? So See the hello world example that will include here when you're generating this comprehensive Of Rst Api Docs. What this allows you to do is very, very quickly iterate on and use the outputs of a language model when all you're trying to get out of a language model is just the text completion For your use case, this L simple decorator was created out of necessity. At generative ai company called ghost, where many, many calls to a language model had to be composed to actually get Zpt 4 to write in a particular tone and. Not hallucinate certain outputs. We were basically having it generate. Completions and then having in other models look at those completions and so on and so forth, and iterating on those completions very rapidly with many, many calls to a language model. 

> Another note to Claude my voice dictation thing is writing L as the letter L.  But I really mean ell. 

A note on this interface for L simple. The idea here is that for multimodal input models, you can still pass in a rich, structured format if you want. So instead of Justice returning a string, you can return a list of messages. And we built this really interesting message. markup language that has a ton of helper functions that are not available in the traditional language model API's from Claude or open AI. Again, because they're generated by stainless And you can pass in very rich, structured inputs that are type coerced in a very nice way. We'll have a note on the message api in just a second. But ultimately, L temple will always return a string. So if you're asking the model to produce structured outputs, function calls, multimodal outputs inherently L dot simple does not support doing this out of the box. The idea is that it is a text only output language model, so you can very rapidly iterate on Simple text output prompts in a very readable way. 


The second decorator type is L complex. And this gives you a very rich structured output from the language model. This is built with multimodal output language models in line. And enables you to have the full complexity of what the language model apis provide for ppt 4O and later versions, as well as applaud son at 3.5 In particular, we'll define a Language model program, which again is just a callable that. defines the system prompt and the messages that the Language model we'll use When it is called and when you. call L dot complex language model program after you've decorated it it returns a message. a message and the message type is our rich Output specification allows. structured outputs Image Outputs audio outputs Pool calling function calling and various other. Supported output modalities for language models The L complex decorator. Provides View a way to specify various tools available to A language model. And structured outputs via pydantic structured output. Pidentic type definitions for the structured outputs and so on. Period 


messagew api
Hello. Testing. Great. A key component of L has been the development of the message. the message API period. Many of the language model providers A specified API for interacting with the inputs and outputs of their language models that comes from a very simple open API spec and this has resulted in the creation of A somewhat cumbersome specification of. Messages. To and from and API. For a large language model At its core. Multimodal language models ingest. ingest and produce A rich structured and markup of. Multimodal content blocks. As a result, when interacting with these models At its most pedantic level. You need a way of traversing this Multimodal document markup. In a clean and effective way When designing this messaging api. We took inspiration from the rich toolkit that exists within the Javascript and HTML ecosystems for interacting with the document markup on web With that said, let's begin describing This api 

Language model messages And completions are Roles and content. Or those are the two fields. The roles are typically an assistant or user role, and sometimes a system role, but generic prompting allows you to use any type of role Content is interesting language. in the language model apis, you get The content can either be a spring or a list of content blocks or. a full result which itself contains contents and a list of content blocks that are either a union of a string or content. All this is safe. It's quite complicated. In our use case A message contains a list of content blocks. And these content blocks can Have nullable fields for the modality. So for example there are content. there are content blocks with fields text which is notable image which is notable audio which is noble tool call which is noble to a result which is noble And structured, which is knowable. These define the various different modalities that language medical input and in the future will output. One of the things that is most frustrating about constructing Messages with the standard api is that there is no. Notion of type coercion. One of the features that is extremely nice about Numpy is that we can type coerce Lists in their operations with each other and. In their construction. Therefore, our message API is built with this as a first class citizen, so you can construct a message from just text from just a string from just a single content block from a list of strings and images and strings again and so on. And this will automatically construct the markup in the correct structure. See examples of how this works and then note to Claude just please include these when you're writing the documentation. Additionally Pulling raw data out of a markup in. traditional open ai. message apis and so on is Quite painful, requiring you to index into content objects Manage indices from choices and completions from models. And then The anti-pattern of type checking. All results in. Somewhat readable or somewhat unreadable, I mean air prone code. In our Framework We've created Helper functions for directly extracting relevant modalities from messages with content blocks and. tool calls and tool results and so on that make Using the complex data output types that these language models produce readable And type safe. For example, we've provided high level API's for extracting the text only from multimodal outputs of language models. The image contents the audio contents and so on from. cached properties on our message objects This allows users to get the full breadth of the L complex decorator and use function calling and tool calling as you'll see in later sections of the documentation, or maybe earlier, depending on how you decide to write this plot. Without It's unreadable. Error-prone output processing code. 

At this point, you probably want to include a bunch of documentation or non-documentation examples in line, you know, showing how you can construct messages and maybe diagrams of the markup. I don't know, but like we want this to feel intuitive. And this section is somewhat related to, like, L dot complex, and it's necessary for us to describe, you know, function falling in all of this stuff in the documentation tool use. chat examples. All of this stuff. 


Another thing to note here for Claude is that just really show exhaustive examples of message coercion. So for like multimodal and text stuff where you do message content and then like just a list of text and then images or you can do content locks or. whatever. Just show all of the message coercion stuff here. No, I know it also need to note about the primary helper functions called L dot assistant L dot user. and L dot system which we can probably auto document from Python so you know threat to RST for that but basically these functions are used for The Basically, our shorthand for message role equals user role equals assistant role ecosystem So definitely note that. 

tool use

OK, let's now talk about tool use. At high level. So the wait tool use works with these large language models. Is always in the context of a chat, so to use tools in L you obviously need to specify tools to a language model, and you need to lcomplexdecorator. The high level flow of tool use with a language model is that you give it access to tools specification. So this is tool name, tool parameters, the return type of the tool. Sometimes And any parameter specifications about which tools it can use when. And then you have a multi turn chat with a language model typically In the multi-turn chat with a language model You provide user messages, and based on those user messages the language model will decide if it's going to use the tool. And if it does, it usually responds in a structured format called a tool call. You then can take these tool calls, and you need to actually, if you want the language model to respond on the basis of executing the tool Call the actual tools that it Asking you to call, collect that into a message, which is a tool result. And then those two results are handled differently by different api providers. For example, anthropic handles tools. by a custom type of content block that contains a tour result, which itself contains content blocks this is very annoying and opening eye has a tool rule Irrespective, you take this tool result message, and you pass it back to the api again, and then. the language model will then provide an output based on the result of that tool to the user This pattern is cumbersome and quite often repeated. And in fact, because API code for these large APIs is automatically generated. Almost all of the post processing code that does exist. Is going to have to be rewritten over and over and over. With an L we Stick with the spirit of Functional language model programming while providing. Easy to use. upper functions to make the tool use process described above very simple period A note to Claude when you're writing the documentation, could you please Add diagram of the tool use flow in the tool using section 

So let's get down to it. How do we actually use tools with L? Well, with L or any language you want to programming, sorry, with any use of tools with large language models, you need to provide a tool specification and handle tool calls and tool results. So how do we do this with L? Well, we specify tools as functions. Schools are functions They are just normal Python functions that are heavily typed and heavily documented and. that types documentation is then provided to the language model in API specific formats. The way you actually specify a tool. Is by defining a function Where all of the arguments are types that A language model can produce and are typed using the python typing api The name of the tool is just the name of the function. The description of the tool provided to the language model is just the doctoring of the function And that's where you will end up prompting the language model based on Your constraints for how it should use the tool. The descriptions of arguments are specified using Native pydantic syntax. So for example, if I have an argument to a tool that says, get the html of a web page, well, I specify, for example, let's say the argument is the website. I can specify and prompt into mirror how the language model should provide that parameter. For example, let's say they should provide the URL without any protocol string like HTTP or HTTPS. I do that by Making the parameter of the function A keyword argument where the default value is a pedantic field Info value, which provides a description and so on and so forth. You can just show this instead of saying it explicitly in the documentation claude. This function, which is fully typed with its decorator having Or sorry, not with its decorator, but with its parameters having all the types and type descriptions. If I need to prompt engineer those and a full dock string for its description as a tool And I make sure that that function, you know, by the way, this is something you have to say that the tool function, whatever you're defining, is a tool for the language model needs to return a string, a Json serializable object for. a list of content blocks. For example, you can have tools in the anthropic api return like an image of maybe like a plot made to the user or like an image and a string and so on and so forth. We're going to have the return type of tool need to have these properties. Then you decorate it with the L dot tool decorator The moment you decorate your tool function, it's automatically versioned and tracked in the same way that language model programs are versioned and tracked. If you're installing Or setting up L with a SQLite store or postpretz store Then to use the tool or have the language model use the tool, you provide the tool to. your L dot complex decorator. This doesn't work with L dot simple because tool calls are not just strings. They are structured output objects that are parsed by the API and so you have to use the complex decorator which allows you to use the messages api or basically the return from a language model program. 


The next component of tool use is once you specify tools to lcomplex via the tools argument. Is the Execution of tool calls and the returning of tool results to your complex language model For use cases that are not multi turn, where you're just simply trying to determine if a tool call must be made based on an input query. If the language model returns a message where message dot toolcalls is not empty, this can be checked with a simple truthy check in Python. Then you can use the helper function Well, I guess in this case you wouldn't use the help function. In that case, then you can go through the tool calls and use an iterator. So for tool intool dot calls Or sorry for tool in message dot tool underscore calls. And do tool dot call. And that should call the tool and give you the result. However You can call Sorry. However, in the multi Turn use case where you have a language model program that takes in a message history and you want to. Have a multi turn chat with Language model program, where you've returned the tool call results You can do the following So suppose we have a tool using chatbot and it's been prompt engineered with this. A message and maybe we go through all the user messages and modify them in some a certain way. And we give it access to the tools and decides to use the tool call. Well, in this case You check toseeifmessage.toolcalls or messages the response from the language model And then you use the helper function message Cole underscore tools underscore and underscore collection. as underscore message and what this will do will go through and call all of your tool calls. You can decide to call them in parallel as well with the parallel arguments you can see here in the example and then it will make a message that contains a tool result. And this tour result can be passed back to the language model. So in a multi turn chat box scenario, you can imagine where every You have a main loop where you check to get a user message. And then if there's a user message, you send it a language model, and then you see if the language model has a tool call message. And if that's the case, then you call the tools, collect this message, send them back to the language model, and then get another response. And that response is usually what the language model interprets the tool calls as. That little text response. And then you continue the loop. This is the sort of use case. You can see the example here, clot to describe this a lot more cleanly than me just voice dictating it to you. 

structured outputs
If you want to use structured outputs with a Large language model Via the LAPI We support this directly in L complex Say for example, you want to 100% guaranteed Chain of thought reasoning using the gpt 4O latest model that supports structured outputs. The best way to do this Is sorry. The way you do this is by specifying the structure and output format with a pydantic model. Pydantic models were used for tool calling and allow you to basically specify all of the various fields of the object you want, the language model to produce As a result of the call. So for example, when the chain of thought reasoning thing we might have a. Recent output, which has a step which is a list of Reasoning steps and a final result, which is a string of the final result. And then we define another model with pi dantic base model, which would be a. Model and that contains The assertion and maybe the proof for it and you have to do that on every single line For your step by step thinking, or whatever your base model is. But you define this return type or response format model and then once you've done this You just pass as an argument to. Lcomplex your response format. In a multi turn conversation, you might want to only have a response format sometime. This is totally fine. You can pass this via the lm params argument 2 a call of language model program. We should talk about this element underscore params argument later Or I think it's api in a square pamps. Sorry. Once you pass that Parameter. Then when you call the language model, it will attempt to produce the structure output This structured output will be contained within the message dot structured Parameter. If a model prefails to produce this output, it will throw an L refusal error which you need to account for Or maybe it won't throw an elbow feels the error, but. Yeah, it's going to throw an L refused layer and the. invocation will fail Structured outputs are as simple as this. When you access the structured object, you will get the structured output. Suppose you don't want to use full json mode and all the various arguments around that Then you can just provide those as parameters to the L complex operator. And best effort attempts to parse the object will happen. And if those fail, you don't obviously get the object on the other side. 


Verbose mode 
We built a simple wrapper for. Printing out The inputs and outputs. So the language model produced by your lineage model programs with verbose mode to set up this you can do L.verbose or L dot config .verbose equals true, or L dot in it with verbose equals true as a parameter, and automatically you can see the inputs and outputs to your language model Calls via calls to your language model programs that were decorated with Elder, simple and elder complex. 

A note on the Albert's simple decorator return type. There are two ways you can specify a simple language model program, ie a language model program that only produces text. When you define your language model program function. The first mode, which is John, stop it. The first mode, which is typically the most readable, but the least pythonic is where you only are defining a system message and a user message in text This is the message cannot be dynamic in this case, but maybe leave this note to later clot. The system message is specified as the doctrine for the function, and the user message is specified as the return string of the function This mode is for very quick prompt engineering, where again you have Text in text out, and you're not typically doing a multi step conversation With a language model The second mode is where your language model program, whether it's simple or complex Only Or not only but specifies a list of. Messages. Via its return statement. So for example, in this case, the dock spring is not used at all. And you define your language on a program by just returning a list of messages. Those messages can be All of the messages that you want to pass, or they can't be, but they're specifically all the messages you're going to pass to your language model api. So in that case, you'll need to specify a system message with L dot system, and your user message with L dot user. Maybe pass the system messages, for example, if you're having a multi turn conversation, or you want to do K shot examples with L dot assistant. And then. those messages using our message api are then converted to the actual message api format that The particular Providers api needs in order to actually execute your language model call This flexibility allows you to build highly readable prompts. While also giving you the ability to. Create complex. Message. API inputs to the linguist model 

llm api params
In out the way that language model api parameters are specified is via. either the decorator or directly on the invocation. For example, if you wanted to have open ai produced And output simultaneously. As is normally available from the chat completion N equals whatever parameter you can. have your language model program in the decorator Specify N5. And what this will do is, when you call your language model program, you. will and the secretary, by the way, works for a simple and complex when you call your language model program, what's going to happen is Langemont program will produce a list instead of an individual output. In the case of L simple, it will produce a list of strings. And in the case of L dot complex, it will produce a list of Messages, which would have been the choices object from open ai. Likewise, for any of the parameters that you would have for that language model, again, you specify those. In the complex operator. For the complex or simple decorators, whatever. So for example, temperature can be specified there and it will work. What if you wanted to change the parameter when you're invoking the Language model program well. after decorating your language model program or function or prompt or whatever the **** it is in the. with the L decorator It adds an additional argument Keyword argument to the function, which is a dictionary of api framers. So you can pass api params dictionary, and then whatever modifications to the default parameter specified. By the decorator that you want. For example, if you wanted a higher temperature output, you would specify temperature in that dictionary. Or if you wanted multiple outputs, you specify N or if you wanted to pass in A structured response format. You could pass it there Currently, we don't support modification To pass in tool calls and various different modality changing parameters. But we will, in the future as well. Period 


mulktimodal inputs
L supports multimodal inputs and multimodal outputs. We separated these two sections because multimodal inputs are available for L simple and by the way, clob when you're writing the documentation, you don't have to write it in these sections, but like we should have a section on multimodality probably. And then subsection for multimodal inputs. The way you specify multimodal inputs is by The type coercion api or not type coercion. The message coercion api is via messages or message coercion. So for example When you define a language model program, you're basically Defining A set of messages that are going to be passed to a language model when you call that program Like we said in the message API, you can do. Messages with multimodal input. So for example, I can define an L simple language model program where. I specify my system messages text in the system prompt, and then I can return What would be the content of the next message or the user message? So that would be maybe. a string, a list of a string with here is my image comma, and then a python image library image, or a numpy array for the image, or a content block with image equals blah, blah, blah . But at a high level Images are just defined. As content blocks with the image property set to either a python image library image or an umpire of the correct shape for an image Or. And this will be implemented in the future, just the base 64 string We also haven't implemented image url yet. That's an open issue. So you can just paste an image in for many image url You can Define these multimodal inputs using the false specification of the message api. So L simple Can either As you know, with L simple instead of defining the program by using the dot string and a return string A return coercive type. What you can do is just have a function that And this works with elders simple or with Lcomplex. You can have a function that returns. A list of messages and. to use multimodality of messages, you do message role equals User. And then you can directly pass in A python image library image. And that will automatically be converted to a message with an image type. Or you can do any of the other sort of coercion steps where you might say, content equals list of and then a string followed by python image library. Or you can have content equals a list of, and then it'll be content block text equals. This is the text content block image equals. And then this is python image library image or a Another numpy array or something like this. Then this automatically gets converted to the open ai message format or the anthropic message format, or whatever the message format of the model provider api is. And you will be able to use multimodal inputs, new line, new line 


Multimodal outputs. Work. In a similar way, but are only accessible via The. Complex. Decorator type, in which case the message will have the message returned will have a property like dot images or audio that contained the output of that model. For now, these functionalities are multi-modal output are not explicitly tested, but upon the release of a gpt 4 Multimodal output API. These will be fully implemented within L. 

Again, no to Claude here. This is pretty poorly described, but basically. All you have to do is just like put a Python image library image or an umpire array of the right shape. In your prompt And then it should work with any multimodal model 


tracing versioning and storage

One of the most important features of L. Is that because all language model programs are Forced to be encapsulated in a functional format. All language model programs, tools And other discrete subroutines that are used in the library can be serialized, versioned And all of their invocations can be tracked and traced as they are used by other language modal programs. Period, this is an extremely powerful feature and happens automatically when Use L dot set store or Linnet with a store directory or log directory specified Underneath the hood, what happens here is that All of the functions that you decorate with an L decorator like simple tool or complex Get Sorry. Yeah, all of them get lexically closed using our. Static and dynamic analysis tooling. That basically computes The source code of the function that's being decorated, plus all of its lexical dependencies. So this is global variables that it depends on, types that it depends on that are not native python types or library types Other language model programs, IE decorated functions that this thing depends on. And that source code is automatically. Serialized And L store. There are many different types of stores. We have postpressed stores. We have sqlite stores and shortly. here we will be implementing a 1B like API service where that You know, it's serialized to, you know, a project on the. L. Studio website, which will be basically a service we offer that looks like one beat and lets you look at all of your linkage model programs. So when you decorate and you set up, when you're decorating all your functions with a store setup, this will automatically Serialize the source code. Of those functions and version them. By computing the hash of the source code as the version. But this also automatically does is builds a complication graph where we can see which language model programs depend on 1 another. So for example, if I have a prompt that depends on another prompt, let's say I generate, I'm trying to write stories, or something like this, and I generate a bunch of story ideas using one prompt, and then I have another thing that basically chooses the best story idea, which is just a critic prompt. And then I finally rewrite the story using, you know, another prompt. If there's basically one. two, three prompts that have a dependency graph, that dependency graph is automatically computed and. that dependency graph will show up within the prompt storage. This feature, I guess we probably want to motivate in this section why this feature matters, but basically like the process of prompt engineering, like we said before, is a form of sort of optimization, right? And in machine learning, we have these things called model checkpoints, which are really powerful And we use a bottle check once, because oftentimes we'll take optimization steps or choose hyperparameters, or choose basically to train and direction on data that maybe later we want to revert, or maybe we want to compare the performance of our prompts over time against some sort of metric, right? It could be a reward model in machine learning. It's some sort of test set, and we're looking at log probabilities. But the high level picture here is we're tracking versions over time. And so we can load those versions up if we made a regression if. we unintendedly change the version of our prompts by like maybe let's say you have a prompt which depends on some random function of your code that uses like a random seed and somebody changed the random seed and now users are complaining of some sort of regression. All of these cases are automatically handled by creating automatic versions of prompts as we do with L and obviously Claude is you're writing the documentation of this. Maybe restructure this with like problem and motivation and then how it works So this is just the version and component of course. So like yeah, the motivation for prompt versioning is at a high level just that. You know you can avoid regressions. You can compare versions to make sure that you're actually hill climbing in prompt space through prompt engineering in the right direction and compare outputs very, very easily And the next part that's really nice is that we can actually, like I said, when you version using sort of an L store, like the Sqlite store, that's just this local tensor board like, you know, tf record style output, where, you know, as I'm prompt engineering, I get, I save all my prompts and indications locally. We can do the same where we actually save indication. So the second part of versioning and this thing is that, hey, not only can we aversion our prompts, but we can look at all of the invocations of our prompts. So calls the language model by version So this is powerful because one. in the process of Well, there's a lot of reasons. One, you might want to fine tune on all these implications later. 2 you might want to compare invocations of a prompt with human labelers Um or with some predefined reward function, like how many you can test that we pass, or something like this 3. We want to look at the latency of prompts by version 4. oftentimes Regressions and failures in code as we're debugging prompts will happen in some unintended way where the input's getting. Mutated wrongly, or something like this, and we can actually look at the inputs to the prompt and sort of what the language model program produced as output to the language model. And then what the language model produced, we can track outputs of the language model over time, usage of the prompt chains, all this stuff. So basically, you know, you can, you have full analytics into all of the prompts and invocations of the prompts that are being used. And that's very good for debugging and all this stuff. And this stuff is automatically already stored in the database in. vacations of prompts with all of the language mono parameters that were used, temperatures, so on and simple. So we have this Automatically When you set up L with a store with L in it, or L set store 
The other thing that's really powerful about L is that when you're actually enabling versioning and indicate. tracking We're actually able to do full Input and output tracing. So When you produce a Output from a language model through a language model program that was developed in L we actually track the invocation that that string came from. So then if I use that string, like so, maybe it's a message from the language model. If I use that in any other prompt, I can see the chain of invocations that flowed through the system. And this happens for free underneath the hood. You don't even have to think about this, so you don't have to structure your props in any which way. But for example, let's say I had a chat Language model program that's just like a simple chat bot with some tools I can trace, you know, the very first invocation of that chat language model program. It's the very 1st user message. And I can see how that user message became part of the message history. And then another message was produced, and so on and so forth and basically automatically compute the thread of messages that occurred. So I can see how conversations evolve over time for various different invocations, because basically any output that's produced by a language model program, so it might be a stir, is wrapped in a typewriter called an underscore Lstr So that's spelled underscore lstr and. that object keeps a frozen set of all of the originator invocations over time and adding a string operator that happens on that thing. So let's say I cut it in half. I do formatting whatever almost any single string operator that happens to that type will preserve the tracing of its originator invocation, and therefore we can actually compute a true computation graph in the style of pi torch the way that sort of. And here's like another thing you could probably bring up as analogies like prompt engineering for machine learning people like this keeps a computation graph keeps track of all of the operations that straight went under as it would pass through the graph. And of course, this is useful for example So many times in practice, when you're actually debugging workflows or trying to see what happens, you want to see how like you know, threads of messages past your language model happen. And it's very hard to do this with. any existing prompt engineering framework out there because they're not really treating this as like a computation graph in the same way that pi torch treats the way tensors flow through different operations as a computation graph because we do this now you can say, oh, like the output of my language model through this chat interface came from here. And before that, it came from here. And before that, it came from here. So we need to sort of illustrate why this is useful. I have pictures. You can just do placeholders and tell me which pictures you need me to get to make this section pop So this is input tracing 


Okay. The other thing we can do now, and this comes package with L with the option auto commit equals true, is auto commit messages on versioning. As your prompt engineering you are changing various parts of the prompt, and you want to diff between Those versions, let's say there was some regression, like a parameter change in the code base, and now your prompt is different, right? That might be the prompt string change. We might have added a rule to the prompt string. That might be The you know some other aspect to the prompt, but something changed And because we serialize the source code of the prompt and all of its dependencies We can use gpt 4O mini, or any other language model, to cheaply and effectively derive a commit message for those changes. So as your prompt engineering over time, you can trace, hey, when I change this parameter, temperature parameter or whatever, the prompts changed in this way. And so you can identify regressions very easily. You can use this to continue iterating in a certain direction And all of this is happening behind the scenes, so you don't need to commit every single time you change the code base. And you can see basically as the code of your prompt changes over time, independent of your code base. This is what was happening. And this is the prompt you settle on, and so on and so forth. This makes a very intuitive interface for prompt engineering, where you can compare different versions of prompts in a natural language way instead of having to look through tons and tons of difs to derive the output and allows us to create interface that looks like this. And then we can show interfaces. 


ell studio
One of the things that we felt was missing from the ecosystem was an open source, locally runable. Prompt engineering. Version control system and dashboard that exists in other disciplines. For example, tensor board with tensor flow. Period. We've built something called L Studio, which is a locally running dashboard you can take and use to inspect a store That you've generated using L or log directory, basically, and visualize all of your prompts, all of the versions of your prompts over time. In real time, look at all of the invocations as they're happening within your system And trace various invocations. Throughout your prompt engineering Our language model programs. El Studio has features that automatically visualize the computation graph of your. prompts and how they depend on one another and how those prompts flow into one another and supports an SQLite backend So that you can have your prompts organized Through various different prompt injuring sessions by just changing the log directory like you would in a deep learning framework like tensorflow. This is extremely useful, again, as not only the prompts are being serialized and stored out automatically and version controlled, but You can, in a local way Visualize Alice prompts are interacting Track latencies, track invocations, total token usage, and other things you might actually find useful in the prompt engineering process. 


Here are the commands to Execute L studio The way to do that is basically, once you install L, you can run L studio and specify the storage dir that you basically implement in your code with your L in it function with Autocommits on or off. And then. it'll open up a server on local host that updates with websockets dynamically do the front end. That will track your invocations, your prompts over time and allow you to sort of introspect that prompt engineering session. If. you wanted to work with Postgres You can modify this server to specify the Postgres database url that you've initialized in your production app Coming soon, we'll have a fully featured Wang B-type service where you can just effectively initialize your store to use our remoteservice and all your prompts will be organized by project, by development mode or production mode For your team to interact with.


